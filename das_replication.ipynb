{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reproducing Distributed Alignment Search (DAS)\n",
    "\n",
    "**Replication of Section 4** from [Geiger et al. (2023)](https://arxiv.org/abs/2303.02536) - \"Finding Alignments Between Interpretable Causal Variables and Distributed Neural Representations\"\n",
    "\n",
    "## Experimental (Re-)Discoveries\n",
    "\n",
    "1. **Part 1 (Random Networks)**: DAS **cannot** construct meaningful causal structure from randomly initialized networks that have not learned the task (~50% IIT accuracy, chance level)\n",
    "\n",
    "2. **Part 2 (Trained Networks)**: DAS **can** discover causal structure in networks trained to solve the hierarchical equality task (~95%+ IIT accuracy)\n",
    "\n",
    "## Methodology\n",
    "\n",
    "- **Task**: Hierarchical equality - determine if (w=x)==(y=z)\n",
    "- **Architecture**: 3-layer feedforward networks (Input→64→64→Output)\n",
    "- **DAS Method**: Learn orthogonal rotation matrices via Interchange Intervention Training (IIT)\n",
    "- **Alignment**: Distributed alignment with V1 (dims 32-48) and V2 (dims 48-64)\n",
    "- **Convergence**: Dev accuracy >= 0.999 or loss delta < 1e-4\n",
    "\n",
    "## Key Results\n",
    "\n",
    "**Random Networks**: ~50% IIT accuracy\n",
    "**Trained Networks**: ~95% IIT accuracy\n",
    "\n",
    "This empirically confirms that DAS discovers pre-existing computational structure and does not construct spurious behaviors, at least for small networks on relatively simple tasks.\n",
    "\n",
    "## Notes\n",
    "Due to dependency issues with `build_graph` in the original repo, this implementation does not use the full DAS infra to reproduce the results. It uses `get_IIT_equality_dataset_both` from the repo to generate the data but trains the rotation matrices and neural networks with a different implementation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fatal: destination path 'InterchangeInterventions' already exists and is not an empty directory.\n",
      "Already on 'zen'\n",
      "Your branch is up to date with 'origin/zen'.\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'matplotlib'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 15\u001b[39m\n\u001b[32m     13\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m     14\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpd\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmatplotlib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpyplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mplt\u001b[39;00m\n\u001b[32m     16\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmetrics\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m accuracy_score\n\u001b[32m     17\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mdataset_equality\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m get_IIT_equality_dataset_both\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'matplotlib'"
     ]
    }
   ],
   "source": [
    "# Cell 1: Clone + Setup + Imports + Configs\n",
    "!git clone https://github.com/atticusg/InterchangeInterventions.git\n",
    "!cd InterchangeInterventions && git checkout zen\n",
    "\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Add the repository to Python path to import DAS modules\n",
    "sys.path.append('InterchangeInterventions')\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score\n",
    "from dataset_equality import get_IIT_equality_dataset_both\n",
    "\n",
    "print(\"All dependencies imported successfully!\")\n",
    "\n",
    "HIDDEN_DIM = 64\n",
    "INPUT_DIM = 16\n",
    "N_CLASSES = 2\n",
    "EMBED_DIM = 4\n",
    "DATASET_SIZE = 10000\n",
    "NUM_NETWORKS = 3\n",
    "RANDOM_SEEDS = [42, 123, 456]\n",
    "\n",
    "# Device\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "    print(\"Using M1 GPU (MPS)\")\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"Using CUDA GPU\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"Using CPU\")\n",
    "\n",
    "print(f\"\\nConfiguration:\")\n",
    "print(f\"  Hidden Size: {HIDDEN_DIM}\")\n",
    "print(f\"  Num Networks: {NUM_NETWORKS}\")\n",
    "print(f\"  Dataset Size: {DATASET_SIZE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Simple 3-Layer Network\n",
    "\n",
    "class SimpleClassifier(nn.Module):\n",
    "    \"\"\"Simple 3-layer feedforward network for hierarchical equality\"\"\"\n",
    "    \n",
    "    def __init__(self, input_dim=16, hidden_dim=64, n_classes=2):\n",
    "        super().__init__()\n",
    "        self.layer1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.layer2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.layer3 = nn.Linear(hidden_dim, n_classes)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.layer1(x))\n",
    "        x = self.relu(self.layer2(x))\n",
    "        x = self.layer3(x)\n",
    "        return x\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"Get class predictions\"\"\"\n",
    "        self.eval()\n",
    "        with torch.no_grad():\n",
    "            # Ensure input is float32 tensor\n",
    "            if isinstance(X, torch.Tensor):\n",
    "                # Convert tensor to float32 if needed\n",
    "                X_tensor = X.float()\n",
    "            else:\n",
    "                # Convert numpy array to float32 tensor\n",
    "                X_tensor = torch.from_numpy(np.asarray(X, dtype=np.float32))\n",
    "\n",
    "            # Move to same device as model\n",
    "            X_tensor = X_tensor.to(next(self.parameters()).device)\n",
    "\n",
    "            outputs = self.forward(X_tensor)\n",
    "            predictions = torch.argmax(outputs, dim=1)\n",
    "\n",
    "        # Return as numpy array\n",
    "        return predictions.cpu().numpy()\n",
    "\n",
    "print(\"Simple 3-layer classifier defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Generate Dataset\n",
    "\n",
    "print(\"Generating hierarchical equality dataset...\")\n",
    "\n",
    "X_base, y_base, X_sources_list, y_IIT, interventions = get_IIT_equality_dataset_both(\n",
    "    embed_dim=EMBED_DIM,\n",
    "    size=DATASET_SIZE\n",
    ")\n",
    "\n",
    "# Split train/test (80/20 split)\n",
    "TRAIN_SIZE = int(0.8 * DATASET_SIZE)\n",
    "TEST_SIZE = DATASET_SIZE - TRAIN_SIZE\n",
    "\n",
    "X_test = X_base[TRAIN_SIZE:]\n",
    "y_test = y_base[TRAIN_SIZE:]\n",
    "y_IIT_test = y_IIT[TRAIN_SIZE:]\n",
    "\n",
    "print(f\"Dataset generated\")\n",
    "print(f\"Total samples: {DATASET_SIZE}\")\n",
    "print(f\"Train samples: {TRAIN_SIZE}\")\n",
    "print(f\"Test samples: {TEST_SIZE}\")\n",
    "print(f\"Base shape: {X_base.shape}\")\n",
    "print(f\"Task: Hierarchical equality (w=x)==(y=z)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Initialize and Evaluate Random Networks\n",
    "\n",
    "print(\"Initializing and evaluating randomly initialized networks...\\n\")\n",
    "\n",
    "results = []\n",
    "\n",
    "for seed in RANDOM_SEEDS:\n",
    "    # Set seed for reproducible random initialization\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    # Create network with RANDOM weights\n",
    "    model = SimpleClassifier(\n",
    "        input_dim=INPUT_DIM,\n",
    "        hidden_dim=HIDDEN_DIM,\n",
    "        n_classes=N_CLASSES\n",
    "    ).to(device)\n",
    "    \n",
    "    # Evaluate on test set\n",
    "    y_pred = model.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    \n",
    "    results.append({\n",
    "        'seed': seed,\n",
    "        'accuracy': accuracy,\n",
    "        'model': model\n",
    "    })\n",
    "    \n",
    "    print(f\"Seed {seed:3d}: Accuracy = {accuracy:.3f}\")\n",
    "\n",
    "avg_accuracy = np.mean([r['accuracy'] for r in results])\n",
    "print(f\"\\nAverage Accuracy: {avg_accuracy:.3f}\")\n",
    "# Check if any network accuracy is more than 3% away from chance (0.5)\n",
    "accuracies = [r['accuracy'] for r in results]\n",
    "deltas = [abs(a - 0.5) for a in accuracies]\n",
    "\n",
    "threshold = 0.03\n",
    "out_indices = [i for i, d in enumerate(deltas) if d > threshold]\n",
    "\n",
    "if out_indices:\n",
    "    print(f\"Networks more than {threshold*100:.0f}% away from chance (50%):\")\n",
    "    # sort by deviation descending for clearer reporting\n",
    "    out_indices.sort(key=lambda i: deltas[i], reverse=True)\n",
    "    for i in out_indices:\n",
    "        r = results[i]\n",
    "        print(f\"  Seed {r['seed']:3d}: Accuracy = {r['accuracy']:.3f} (deviation = {deltas[i]:.3f})\")\n",
    "else:\n",
    "    print(\"All networks within ±3% of chance (50%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Train Rotation Matrices with Distributed Interchange Interventions\n",
    "\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "print(\"Training orthogonal rotation matrices with DAS...\\n\")\n",
    "print(\"Goal: Learn rotations where swapping V1 and V2 in rotated space\")\n",
    "print(\"corresponds to swapping causal variables in the high-level model\\n\")\n",
    "\n",
    "EPOCHS = 10\n",
    "LEARNING_RATE = 0.001\n",
    "BATCH_SIZE = 512\n",
    "\n",
    "# For hidden_dim=64 with distributed alignment:\n",
    "# - Dimensions 0-32: Base representation (not intervened)\n",
    "# - Dimensions 32-48: V1 (left pair equality)\n",
    "# - Dimensions 48-64: V2 (right pair equality)\n",
    "V1_START, V1_END = 32, 48\n",
    "V2_START, V2_END = 48, 64\n",
    "\n",
    "for i, result in enumerate(results):\n",
    "    model = result['model']\n",
    "    seed = result['seed']\n",
    "    \n",
    "    print(f\"Training network {seed} ({i+1}/{NUM_NETWORKS})...\")\n",
    "    \n",
    "    # Freeze all model weights - only rotation is trainable\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = False\n",
    "    \n",
    "    # Create rotation matrix for layer 1 activations\n",
    "    # Initialize on CPU (MPS doesn't support QR decomposition), then move to device\n",
    "    rotation = nn.Linear(HIDDEN_DIM, HIDDEN_DIM, bias=False)\n",
    "    nn.init.orthogonal_(rotation.weight)\n",
    "    rotation = rotation.to(device)\n",
    "    \n",
    "    # Optimizer for rotation only\n",
    "    optimizer = optim.Adam(rotation.parameters(), lr=LEARNING_RATE)\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    \n",
    "    # Prepare training data (convert to float32)\n",
    "    X_train = X_base[:TRAIN_SIZE]\n",
    "    y_train_iit = y_IIT[:TRAIN_SIZE]  # Pre-computed IIT labels\n",
    "    X_source1_train = X_sources_list[0][:TRAIN_SIZE]\n",
    "    X_source2_train = X_sources_list[1][:TRAIN_SIZE]\n",
    "    \n",
    "    # Create dataset and dataloader for batching\n",
    "    dataset = TensorDataset(\n",
    "        X_train.float(),\n",
    "        y_train_iit.long(),\n",
    "        X_source1_train.float(),\n",
    "        X_source2_train.float()\n",
    "    )\n",
    "    dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    \n",
    "    # Training loop\n",
    "    for epoch in range(EPOCHS):\n",
    "        epoch_loss = 0.0\n",
    "        num_batches = 0\n",
    "        \n",
    "        for batch_base, batch_y_iit, batch_source1, batch_source2 in dataloader:\n",
    "            # Move batch to device\n",
    "            batch_base = batch_base.to(device)\n",
    "            batch_y_iit = batch_y_iit.to(device)\n",
    "            batch_source1 = batch_source1.to(device)\n",
    "            batch_source2 = batch_source2.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # === DISTRIBUTED INTERCHANGE INTERVENTION ===\n",
    "            \n",
    "            # 1. Get layer 1 activations for base and sources\n",
    "            with torch.no_grad():\n",
    "                base_h1 = model.relu(model.layer1(batch_base))  # [batch, 64]\n",
    "                source1_h1 = model.relu(model.layer1(batch_source1))\n",
    "                source2_h1 = model.relu(model.layer1(batch_source2))\n",
    "            \n",
    "            # 2. Apply rotation to transform to interpretable space\n",
    "            base_h1_rotated = rotation(base_h1)\n",
    "            source1_h1_rotated = rotation(source1_h1)\n",
    "            source2_h1_rotated = rotation(source2_h1)\n",
    "            \n",
    "            # 3. Perform interchange intervention in rotated space\n",
    "            # Keep base dimensions (0:32) unchanged\n",
    "            # Replace V1 region (dims 32:48) with source1's V1\n",
    "            # Replace V2 region (dims 48:64) with source2's V2\n",
    "            intervened_h1_rotated = base_h1_rotated.clone()\n",
    "            intervened_h1_rotated[:, V1_START:V1_END] = source1_h1_rotated[:, V1_START:V1_END]\n",
    "            intervened_h1_rotated[:, V2_START:V2_END] = source2_h1_rotated[:, V2_START:V2_END]\n",
    "            \n",
    "            # 4. Apply inverse rotation to return to original activation space\n",
    "            # For orthogonal matrices: R^{-1} = R^T\n",
    "            intervened_h1 = torch.matmul(intervened_h1_rotated, rotation.weight.T)\n",
    "            \n",
    "            # 5. Continue forward pass through remaining layers\n",
    "            h2 = model.relu(model.layer2(intervened_h1))\n",
    "            outputs = model.layer3(h2)\n",
    "            \n",
    "            # 6. Compute loss against IIT labels\n",
    "            # IIT labels are pre-computed: whether source1's left pair equality\n",
    "            # matches source2's right pair equality\n",
    "            loss = loss_fn(outputs, batch_y_iit)\n",
    "            \n",
    "            # 7. Update rotation matrix only\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # 8. Re-orthogonalize rotation matrix after each update (SVD projection)\n",
    "            # This maintains orthogonality during training\n",
    "            with torch.no_grad():\n",
    "                # Move to CPU for SVD (MPS doesn't support it)\n",
    "                W = rotation.weight.data.cpu()\n",
    "                U, _, Vt = torch.linalg.svd(W, full_matrices=False)\n",
    "                rotation.weight.data = (U @ Vt).to(device)\n",
    "            \n",
    "            epoch_loss += loss.item()\n",
    "            num_batches += 1\n",
    "        \n",
    "        avg_loss = epoch_loss / num_batches\n",
    "        if (epoch + 1) % 2 == 0:\n",
    "            print(f\"  Epoch {epoch+1}/{EPOCHS}, Loss: {avg_loss:.4f}\")\n",
    "    \n",
    "    result['rotation'] = rotation\n",
    "    print(f\"Training complete\\n\")\n",
    "\n",
    "print(\"Rotation matrix training complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Evaluate Interchange Intervention Accuracy\n",
    "\n",
    "print(\"Evaluating interchange intervention accuracy after rotation training...\\n\")\n",
    "\n",
    "for result in results:\n",
    "    model = result['model']\n",
    "    rotation = result['rotation']\n",
    "    seed = result['seed']\n",
    "    \n",
    "    model.eval()\n",
    "    rotation.eval()\n",
    "    \n",
    "    # Prepare test data (convert to float32)\n",
    "    X_test_tensor = X_test.float().to(device)\n",
    "    X_source1_test = X_sources_list[0][TRAIN_SIZE:].float().to(device)\n",
    "    X_source2_test = X_sources_list[1][TRAIN_SIZE:].float().to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # === SAME INTERCHANGE INTERVENTION AS TRAINING ===\n",
    "        \n",
    "        # 1. Get layer 1 activations\n",
    "        base_h1 = model.relu(model.layer1(X_test_tensor))\n",
    "        source1_h1 = model.relu(model.layer1(X_source1_test))\n",
    "        source2_h1 = model.relu(model.layer1(X_source2_test))\n",
    "        \n",
    "        # 2. Apply rotation\n",
    "        base_h1_rotated = rotation(base_h1)\n",
    "        source1_h1_rotated = rotation(source1_h1)\n",
    "        source2_h1_rotated = rotation(source2_h1)\n",
    "        \n",
    "        # 3. Swap V1 and V2 in rotated space (same coordinates as training)\n",
    "        intervened_h1_rotated = base_h1_rotated.clone()\n",
    "        intervened_h1_rotated[:, 32:48] = source1_h1_rotated[:, 32:48]   # V1 from source1\n",
    "        intervened_h1_rotated[:, 48:64] = source2_h1_rotated[:, 48:64]   # V2 from source2\n",
    "        \n",
    "        # 4. Apply inverse rotation\n",
    "        intervened_h1 = torch.matmul(intervened_h1_rotated, rotation.weight.T)\n",
    "        \n",
    "        # 5. Continue through network\n",
    "        h2 = model.relu(model.layer2(intervened_h1))\n",
    "        outputs = model.layer3(h2)\n",
    "        \n",
    "        predictions = torch.argmax(outputs, dim=1).cpu().numpy()\n",
    "    \n",
    "    # Compute IIT accuracy against pre-computed labels\n",
    "    iit_accuracy = accuracy_score(y_IIT_test, predictions)\n",
    "    result['iit_accuracy'] = iit_accuracy\n",
    "    \n",
    "    print(f\"Seed {seed:3d}: IIT Accuracy = {iit_accuracy:.3f}\")\n",
    "\n",
    "avg_iit_accuracy = np.mean([r['iit_accuracy'] for r in results])\n",
    "print(f\"\\nAverage IIT Accuracy: {avg_iit_accuracy:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: Train Networks on Task\n",
    "\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"Training Networks on Hierarchical Equality Task\")\n",
    "print(\"=\" * 60)\n",
    "print()\n",
    "\n",
    "CHECKPOINT_FILE = 'trained_networks.pkl'\n",
    "\n",
    "# Generate test dataset\n",
    "print(\"Generating test dataset (10,000 samples)...\")\n",
    "X_test_task, y_test_task, _, _, _ = get_IIT_equality_dataset_both(embed_dim=EMBED_DIM, size=10000)\n",
    "\n",
    "# Check if checkpoint exists\n",
    "if os.path.exists(CHECKPOINT_FILE):\n",
    "    print(f\"Found checkpoint {CHECKPOINT_FILE}, loading trained networks...\")\n",
    "    \n",
    "    with open(CHECKPOINT_FILE, 'rb') as f:\n",
    "        trained_models = pickle.load(f)\n",
    "    \n",
    "    print(\"Loaded trained networks from checkpoint\\n\")\n",
    "    \n",
    "else:\n",
    "    print(\"No checkpoint found, training from scratch\\n\")\n",
    "    \n",
    "    # Generate fresh training dataset\n",
    "    print(\"Generating training dataset (50,000 samples)...\")\n",
    "    X_train_full, y_train_full, _, _, _ = get_IIT_equality_dataset_both(embed_dim=EMBED_DIM, size=50000)\n",
    "    \n",
    "    # Split into train/dev (90/10)\n",
    "    train_size = int(0.9 * len(X_train_full))\n",
    "    X_train_task = X_train_full[:train_size]\n",
    "    y_train_task = y_train_full[:train_size]\n",
    "    X_dev_task = X_train_full[train_size:]\n",
    "    y_dev_task = y_train_full[train_size:]\n",
    "    \n",
    "    print(f\"Train: {len(X_train_task)}, Dev: {len(X_dev_task)}, Test: {len(X_test_task)}\\n\")\n",
    "    \n",
    "    TASK_EPOCHS = 30\n",
    "    TASK_BATCH_SIZE = 128\n",
    "    TASK_LR = 0.001\n",
    "    CONVERGENCE_DELTA = 1e-4\n",
    "    CONVERGENCE_ACC = 0.999\n",
    "    \n",
    "    trained_models = {}\n",
    "    \n",
    "    for seed in RANDOM_SEEDS:\n",
    "        print(f\"Training Network {seed}...\")\n",
    "        torch.manual_seed(seed)\n",
    "        np.random.seed(seed)\n",
    "        \n",
    "        model = SimpleClassifier(INPUT_DIM, HIDDEN_DIM, N_CLASSES).to(device)\n",
    "        optimizer = optim.Adam(model.parameters(), lr=TASK_LR)\n",
    "        loss_fn = nn.CrossEntropyLoss()\n",
    "        \n",
    "        dataset = TensorDataset(X_train_task.float(), y_train_task.long())\n",
    "        dataloader = DataLoader(dataset, batch_size=TASK_BATCH_SIZE, shuffle=True)\n",
    "        \n",
    "        prev_loss = float('inf')\n",
    "        \n",
    "        for epoch in range(TASK_EPOCHS):\n",
    "            model.train()\n",
    "            epoch_loss = 0.0\n",
    "            num_batches = 0\n",
    "            \n",
    "            for batch_x, batch_y in dataloader:\n",
    "                batch_x = batch_x.to(device)\n",
    "                batch_y = batch_y.to(device)\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(batch_x)\n",
    "                loss = loss_fn(outputs, batch_y)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "                epoch_loss += loss.item()\n",
    "                num_batches += 1\n",
    "            \n",
    "            avg_epoch_loss = epoch_loss / num_batches\n",
    "            \n",
    "            # Evaluate on dev set every 5 epochs\n",
    "            if (epoch + 1) % 5 == 0:\n",
    "                model.eval()\n",
    "                with torch.no_grad():\n",
    "                    dev_preds = model.predict(X_dev_task)\n",
    "                    dev_acc = accuracy_score(y_dev_task, dev_preds)\n",
    "                \n",
    "                loss_delta = abs(prev_loss - avg_epoch_loss)\n",
    "                print(f\"Epoch {epoch+1:3d}: Loss={avg_epoch_loss:.4f}, Dev Acc={dev_acc:.4f}, ΔLoss={loss_delta:.6f}\")\n",
    "                \n",
    "                # Convergence criteria\n",
    "                if dev_acc >= CONVERGENCE_ACC:\n",
    "                    print(f\"Converged: Dev accuracy >= {CONVERGENCE_ACC}\")\n",
    "                    break\n",
    "                if loss_delta < CONVERGENCE_DELTA:\n",
    "                    print(f\"Converged: Loss change < {CONVERGENCE_DELTA}\")\n",
    "                    break\n",
    "                \n",
    "                prev_loss = avg_epoch_loss\n",
    "        \n",
    "        # Move model to CPU for serialization\n",
    "        trained_models[seed] = model.cpu().state_dict()\n",
    "        print()\n",
    "    \n",
    "    # Save checkpoint\n",
    "    print(f\"Saving checkpoint to {CHECKPOINT_FILE}...\")\n",
    "    with open(CHECKPOINT_FILE, 'wb') as f:\n",
    "        pickle.dump(trained_models, f)\n",
    "    print(\"Checkpoint saved\\n\")\n",
    "\n",
    "# Evaluate all models on test set\n",
    "print(\"Evaluating networks on test set...\")\n",
    "trained_results = []\n",
    "\n",
    "for seed in RANDOM_SEEDS:\n",
    "    model = SimpleClassifier(INPUT_DIM, HIDDEN_DIM, N_CLASSES)\n",
    "    model.load_state_dict(trained_models[seed])\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    final_test_acc = accuracy_score(y_test_task, model.predict(X_test_task))\n",
    "    \n",
    "    trained_results.append({\n",
    "        'seed': seed,\n",
    "        'model': model,\n",
    "        'task_accuracy': final_test_acc\n",
    "    })\n",
    "    \n",
    "    print(f\"Network {seed}: Test Accuracy = {final_test_acc:.4f}\")\n",
    "\n",
    "avg_task_acc = np.mean([r['task_accuracy'] for r in trained_results])\n",
    "print(f\"\\n{'─' * 60}\")\n",
    "print(f\"Average Test Accuracy: {avg_task_acc:.4f}\")\n",
    "print(f\"{'─' * 60}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8: DAS on Trained Networks\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"DAS Training on Trained Networks\")\n",
    "print(\"=\" * 60)\n",
    "print()\n",
    "\n",
    "ROTATION_CHECKPOINT = 'trained_rotations.pkl'\n",
    "\n",
    "# Check if rotation checkpoint exists\n",
    "if os.path.exists(ROTATION_CHECKPOINT):\n",
    "    print(f\"Found rotation checkpoint {ROTATION_CHECKPOINT}, loading...\")\n",
    "    \n",
    "    with open(ROTATION_CHECKPOINT, 'rb') as f:\n",
    "        rotation_states = pickle.load(f)\n",
    "    \n",
    "    print(\"Loaded rotation matrices from checkpoint\\n\")\n",
    "    \n",
    "else:\n",
    "    print(\"No rotation checkpoint found, training from scratch\\n\")\n",
    "    \n",
    "    print(\"Generating IIT dataset (8,000 samples)...\")\n",
    "    X_iit, y_iit_base, X_src_iit, y_iit, int_iit = get_IIT_equality_dataset_both(embed_dim=EMBED_DIM, size=8000)\n",
    "    print()\n",
    "    \n",
    "    rotation_states = {}\n",
    "    \n",
    "    for i, result in enumerate(trained_results):\n",
    "        model, seed = result['model'], result['seed']\n",
    "        print(f\"Training DAS for Network {seed} ({i+1}/{NUM_NETWORKS})...\")\n",
    "        \n",
    "        for param in model.parameters():\n",
    "            param.requires_grad = False\n",
    "        \n",
    "        rotation = nn.Linear(HIDDEN_DIM, HIDDEN_DIM, bias=False)\n",
    "        nn.init.orthogonal_(rotation.weight)\n",
    "        rotation = rotation.to(device)\n",
    "        \n",
    "        optimizer = optim.Adam(rotation.parameters(), lr=0.001)\n",
    "        loss_fn = nn.CrossEntropyLoss()\n",
    "        \n",
    "        dataset = TensorDataset(X_iit.float(), y_iit.long(), X_src_iit[0].float(), X_src_iit[1].float())\n",
    "        dataloader = DataLoader(dataset, batch_size=64, shuffle=True)\n",
    "        \n",
    "        for epoch in range(10):\n",
    "            epoch_loss = 0\n",
    "            for batch_base, batch_y, batch_s1, batch_s2 in dataloader:\n",
    "                batch_base, batch_y = batch_base.to(device), batch_y.to(device)\n",
    "                batch_s1, batch_s2 = batch_s1.to(device), batch_s2.to(device)\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                with torch.no_grad():\n",
    "                    h1_base = model.relu(model.layer1(batch_base))\n",
    "                    h1_s1 = model.relu(model.layer1(batch_s1))\n",
    "                    h1_s2 = model.relu(model.layer1(batch_s2))\n",
    "                \n",
    "                h1_base_rot = rotation(h1_base)\n",
    "                h1_s1_rot = rotation(h1_s1)\n",
    "                h1_s2_rot = rotation(h1_s2)\n",
    "                \n",
    "                h1_int = h1_base_rot.clone()\n",
    "                h1_int[:, 32:48] = h1_s1_rot[:, 32:48]\n",
    "                h1_int[:, 48:64] = h1_s2_rot[:, 48:64]\n",
    "                \n",
    "                h1_back = torch.matmul(h1_int, rotation.weight.T)\n",
    "                h2 = model.relu(model.layer2(h1_back))\n",
    "                outputs = model.layer3(h2)\n",
    "                \n",
    "                loss = loss_fn(outputs, batch_y)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "                with torch.no_grad():\n",
    "                    W = rotation.weight.data.cpu()\n",
    "                    U, _, Vt = torch.linalg.svd(W, full_matrices=False)\n",
    "                    rotation.weight.data = (U @ Vt).to(device)\n",
    "                \n",
    "                epoch_loss += loss.item()\n",
    "            \n",
    "            if (epoch + 1) % 5 == 0:\n",
    "                print(f\"Epoch {epoch+1}, Loss: {epoch_loss / len(dataloader):.4f}\")\n",
    "        \n",
    "        rotation_states[seed] = rotation.cpu().state_dict()\n",
    "        print()\n",
    "    \n",
    "    # Save rotation checkpoint\n",
    "    print(f\"Saving rotation checkpoint to {ROTATION_CHECKPOINT}...\")\n",
    "    with open(ROTATION_CHECKPOINT, 'wb') as f:\n",
    "        pickle.dump(rotation_states, f)\n",
    "    print(\"Checkpoint saved\\n\")\n",
    "\n",
    "# Load rotations and attach to results\n",
    "for r in trained_results:\n",
    "    rotation = nn.Linear(HIDDEN_DIM, HIDDEN_DIM, bias=False)\n",
    "    rotation.load_state_dict(rotation_states[r['seed']])\n",
    "    rotation = rotation.to(device)\n",
    "    rotation.eval()\n",
    "    r['rotation'] = rotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 9: Evaluate Trained Networks\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"IIT Accuracy on Trained Networks\")\n",
    "print(\"=\" * 60)\n",
    "print()\n",
    "\n",
    "print(\"Generating test IIT dataset (2,000 samples)...\")\n",
    "X_iit_t, y_iit_t_base, X_src_t, y_iit_t, int_t = get_IIT_equality_dataset_both(embed_dim=EMBED_DIM, size=2000)\n",
    "print(\"Ready\\n\")\n",
    "\n",
    "for result in trained_results:\n",
    "    model, rotation, seed = result['model'], result['rotation'], result['seed']\n",
    "    model.eval()\n",
    "    rotation.eval()\n",
    "    \n",
    "    X_t = X_iit_t.float().to(device)\n",
    "    X_s1 = X_src_t[0].float().to(device)\n",
    "    X_s2 = X_src_t[1].float().to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        h1_base = model.relu(model.layer1(X_t))\n",
    "        h1_s1 = model.relu(model.layer1(X_s1))\n",
    "        h1_s2 = model.relu(model.layer1(X_s2))\n",
    "        \n",
    "        h1_base_rot = rotation(h1_base)\n",
    "        h1_s1_rot = rotation(h1_s1)\n",
    "        h1_s2_rot = rotation(h1_s2)\n",
    "        \n",
    "        h1_int = h1_base_rot.clone()\n",
    "        h1_int[:, 32:48] = h1_s1_rot[:, 32:48]\n",
    "        h1_int[:, 48:64] = h1_s2_rot[:, 48:64]\n",
    "        \n",
    "        h1_back = torch.matmul(h1_int, rotation.weight.T)\n",
    "        h2 = model.relu(model.layer2(h1_back))\n",
    "        outputs = model.layer3(h2)\n",
    "        preds = torch.argmax(outputs, dim=1).cpu().numpy()\n",
    "    \n",
    "    iit_acc = accuracy_score(y_iit_t, preds)\n",
    "    result['iit_accuracy'] = iit_acc\n",
    "    \n",
    "    print(f\"Network {seed}: Task={result['task_accuracy']:.4f}, IIT={iit_acc:.4f}\")\n",
    "\n",
    "avg_trained_iit = np.mean([r['iit_accuracy'] for r in trained_results])\n",
    "print(f\"\\nAverage IIT: {avg_trained_iit:.4f}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 10: Final Comparison with Visualization\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"FINAL COMPARISON\")\n",
    "print(\"=\" * 60)\n",
    "print()\n",
    "\n",
    "# Print summary\n",
    "print(\"RANDOM NETWORKS:\")\n",
    "for r in results:\n",
    "    print(f\"  {r['seed']}: Task={r['accuracy']:.3f}, IIT={r['iit_accuracy']:.3f}\")\n",
    "\n",
    "print(f\"\\nTRAINED NETWORKS:\")\n",
    "for r in trained_results:\n",
    "    print(f\"  {r['seed']}: Task={r['task_accuracy']:.3f}, IIT={r['iit_accuracy']:.3f}\")\n",
    "\n",
    "print(f\"\\n{'─' * 60}\")\n",
    "print(f\"Random  - Avg IIT: {avg_iit_accuracy:.3f} (chance)\")\n",
    "print(f\"Trained - Avg IIT: {avg_trained_iit:.3f} (learned structure)\")\n",
    "print(f\"Improvement: {(avg_trained_iit - avg_iit_accuracy):.3f}\")\n",
    "print(f\"{'─' * 60}\\n\")\n",
    "\n",
    "# Create visualization\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Data\n",
    "networks = [str(s) for s in RANDOM_SEEDS]\n",
    "random_task = [r['accuracy'] for r in results]\n",
    "random_iit = [r['iit_accuracy'] for r in results]\n",
    "trained_task = [r['task_accuracy'] for r in trained_results]\n",
    "trained_iit = [r['iit_accuracy'] for r in trained_results]\n",
    "\n",
    "x = range(len(networks))\n",
    "width = 0.35\n",
    "\n",
    "# Left panel: Task Accuracy\n",
    "ax1.bar([i - width/2 for i in x], random_task, width, label='Random', alpha=0.8, color='#e74c3c')\n",
    "ax1.bar([i + width/2 for i in x], trained_task, width, label='Trained', alpha=0.8, color='#2ecc71')\n",
    "ax1.axhline(y=0.5, color='gray', linestyle='--', linewidth=1, label='Chance (50%)')\n",
    "ax1.set_ylabel('Task Accuracy', fontsize=12, fontweight='bold')\n",
    "ax1.set_xlabel('Network Seed', fontsize=12)\n",
    "ax1.set_title('Task Performance', fontsize=14, fontweight='bold')\n",
    "ax1.set_xticks(x)\n",
    "ax1.set_xticklabels(networks)\n",
    "ax1.legend(fontsize=10)\n",
    "ax1.set_ylim([0, 1.05])\n",
    "ax1.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Right panel: IIT Accuracy\n",
    "ax2.bar([i - width/2 for i in x], random_iit, width, label='Random', alpha=0.8, color='#e74c3c')\n",
    "ax2.bar([i + width/2 for i in x], trained_iit, width, label='Trained', alpha=0.8, color='#2ecc71')\n",
    "ax2.axhline(y=0.5, color='gray', linestyle='--', linewidth=1, label='Chance (50%)')\n",
    "ax2.set_ylabel('IIT Accuracy', fontsize=12, fontweight='bold')\n",
    "ax2.set_xlabel('Network Seed', fontsize=12)\n",
    "ax2.set_title('DAS Interchange Intervention Accuracy', fontsize=14, fontweight='bold')\n",
    "ax2.set_xticks(x)\n",
    "ax2.set_xticklabels(networks)\n",
    "ax2.legend(fontsize=10)\n",
    "ax2.set_ylim([0, 1.05])\n",
    "ax2.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Add text annotations showing averages\n",
    "ax1.text(0.5, 0.95, f'Random avg: {avg_accuracy:.3f}', \n",
    "         transform=ax1.transAxes, ha='center', fontsize=10, \n",
    "         bbox=dict(boxstyle='round', facecolor='#e74c3c', alpha=0.3))\n",
    "ax1.text(0.5, 0.88, f'Trained avg: {avg_task_acc:.3f}', \n",
    "         transform=ax1.transAxes, ha='center', fontsize=10,\n",
    "         bbox=dict(boxstyle='round', facecolor='#2ecc71', alpha=0.3))\n",
    "\n",
    "ax2.text(0.5, 0.95, f'Random avg: {avg_iit_accuracy:.3f}', \n",
    "         transform=ax2.transAxes, ha='center', fontsize=10,\n",
    "         bbox=dict(boxstyle='round', facecolor='#e74c3c', alpha=0.3))\n",
    "ax2.text(0.5, 0.88, f'Trained avg: {avg_trained_iit:.3f}', \n",
    "         transform=ax2.transAxes, ha='center', fontsize=10,\n",
    "         bbox=dict(boxstyle='round', facecolor='#2ecc71', alpha=0.3))\n",
    "\n",
    "plt.suptitle('DAS Performance: Random vs Trained Networks', \n",
    "             fontsize=16, fontweight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save figure\n",
    "plt.savefig('das-comparison.png', dpi=300, bbox_inches='tight')\n",
    "print(\"✓ Plot saved to das-comparison.png\")\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "das-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
